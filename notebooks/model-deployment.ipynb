{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af5db85-8396-48d5-bcb3-4c20b0aa0187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdc9cc24847431e85f6d4ed0541979b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login required for the first time running this notebook\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b75245-fe11-4f7b-8bb1-ff9e6b1893d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0437999-2e18-46f4-bbb2-65c50b70f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "from IPython.display import Audio,display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e800ea-9760-4bf4-90a3-1661fabd400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio model not found, fetching from hugging face\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fe848b327a4a639eb3546e5dcdf155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'SpeechT5ForTextToSpeech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[1;32m     11\u001b[0m notebook_login()\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSpeechT5ForTextToSpeech\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNour17/speecht5_finetuned_Andrew_NG_complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SpeechT5ForTextToSpeech' is not defined"
     ]
    }
   ],
   "source": [
    "# load model from huggingface or local if found\n",
    "audio_model_path = 'audio_model'\n",
    "if os.path.exists(audio_model_path):\n",
    "    print(\"Audio model found\")\n",
    "    model = SpeechT5ForTextToSpeech.from_pretrained('audio_model')\n",
    "else:\n",
    "    print(\"Audio model not found, fetching from hugging face\")\n",
    "    model = SpeechT5ForTextToSpeech.from_pretrained(\"Nour17/speecht5_finetuned_Andrew_NG_complete\")\n",
    "    model.save_pretrained('audio_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d6e6ce-f1d1-48d0-926e-bb46c311a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio dataset found\n"
     ]
    }
   ],
   "source": [
    "# load dataset from huggingface or local if found\n",
    "dataset_model_path = 'audio_dataset'\n",
    "if os.path.exists(dataset_model_path):\n",
    "    print(\"Audio dataset found\")\n",
    "    dataset = load_from_disk('audio_dataset')\n",
    "else:\n",
    "    print(\"Audio dataset not found, fetching from hugging face\")\n",
    "    dataset = load_dataset(\"Nour17/Andrew_NG_audio_dataset\")\n",
    "    dataset.save_to_disk('audio_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b139763-09db-4467-b26e-e0ff197e3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default speaker embeddings\n",
    "import torch\n",
    "example = dataset[\"test\"][0]\n",
    "default_speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0669e35-cd05-4f6b-bf5f-64510b0b8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n"
     ]
    }
   ],
   "source": [
    "# normalize input text \n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "normalizer = Normalizer(input_case='cased', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5266286b-5bdf-450f-985a-52b289040ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mel-spectogram to audio waveform model (reverse fft phase shift)\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f378d5d-89ce-4110-a2ea-84669d63277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences and add period at the end of each one\n",
    "def get_sentences(text, add_period = True):\n",
    "    period = \".\" if add_period else \"\"\n",
    "    sentences = [sent + period for sent in text.split(\".\")]\n",
    "    # remove last period if it exsists and add_period flag is True\n",
    "    if sentences[-1] == period : sentences = sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "# divide large text into sentences chunks\n",
    "def chunk_text(text, max_length = 400):\n",
    "    sentences = get_sentences(text)\n",
    "    chunks = [[]]\n",
    "    chunks_idx = 0\n",
    "    curr_length = 0\n",
    "    for sent in sentences:\n",
    "        curr_length += len(sent)\n",
    "        if (curr_length < max_length):\n",
    "            chunks[chunks_idx].append(sent)\n",
    "        else:\n",
    "            chunks.append([sent])\n",
    "            chunks_idx += 1\n",
    "            curr_length = len(sent)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57d10a2-27ba-4032-b4d4-b828b65fd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(sentences):\n",
    "    sentences = normalizer.normalize_list(sentences)\n",
    "    inputs = processor(text=sentences,is_split_into_words=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "def generate_single_speech(text):\n",
    "    if type(text) == list :\n",
    "        inputs = prepare_sentences(text)\n",
    "        speech_tensor = model.generate_speech(inputs[\"input_ids\"], default_speaker_embeddings, vocoder=vocoder)\n",
    "    elif type(text) == str :\n",
    "        sentences = get_sentences(text)\n",
    "        print(sentences)\n",
    "        inputs = processor(text=normalizer.normalize(sentences[0]), return_tensors=\"pt\") if len(sentences) == 1 else prepare_sentences(sentences)\n",
    "    speech_tensor = model.generate_speech(inputs[\"input_ids\"], default_speaker_embeddings, vocoder=vocoder)\n",
    "    return speech_tensor.numpy()\n",
    "\n",
    "def generate_speech(text):\n",
    "    transcripts = chunk_text(text)\n",
    "    speech_tensor_list = []\n",
    "    for transcript in transcripts:\n",
    "        print(transcript)\n",
    "        speech_tensor_list.append(generate_single_speech(transcript))\n",
    "    return np.concatenate(speech_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2cdd985-b9c4-4cc4-8716-a6bbb044d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pydub import AudioSegment\n",
    "from moviepy.editor import ImageClip,AudioFileClip, concatenate_videoclips\n",
    "from moviepy.audio.AudioClip import AudioArrayClip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Slide:\n",
    "    def __init__(self, title, bullet_points, audio_array,audio_path):\n",
    "        self.title = title\n",
    "        self.bullet_points = bullet_points\n",
    "        # reshape audio array to be size (N x 1) if necessary\n",
    "        self.audio_array = audio_array if len(audio_array.shape) == 2 else audio_array.reshape(audio_array.shape[0],1)\n",
    "        self.audio_path = audio_path\n",
    "\n",
    "    def generate_img(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(self.title)\n",
    "        for idx, bp in enumerate(self.bullet_points):\n",
    "            ax.text(0.05, 0.9 - idx * 0.1, f\"- {bp}\", fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off') \n",
    "\n",
    "        # Convert Matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_data = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "        plt.close()\n",
    "        return img_data\n",
    "\n",
    "    \"\"\"def get_slide_duration(self):\n",
    "        audio = AudioSegment.from_file(self.audio_path)\n",
    "        return len(audio)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_video_from_slides(slides,video_name):\n",
    "        clips = []\n",
    "        for slide in slides:\n",
    "            #slide_duration = slide.get_slide_duration()\n",
    "            slide_img = slide.generate_img()\n",
    "            #audio_clip = AudioArrayClip(slide.audio_array,fps = 32000)\n",
    "            audio_clip = AudioFileClip(slide.audio_path)\n",
    "            slide_clip = ImageClip(slide_img,duration = audio_clip.duration)\n",
    "            slide_clip = slide_clip.set_audio(audio_clip)\n",
    "            print(slide_clip.duration)\n",
    "            clips.append(slide_clip)\n",
    "        final_clip = concatenate_videoclips(clips)\n",
    "        # export video\n",
    "        final_clip.write_videofile(video_name, fps=24)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c4c4b84-b9e2-4f2f-8e09-314c936e5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smapling rate constant\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed030a9-c912-4740-b6f6-7d57efcaab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "def prediction(title: str,points:[str],text:str):\n",
    "    speech = generate_speech(text)\n",
    "    filename = title + '.wav'\n",
    "    write(filename, sr, speech)\n",
    "    slide = Slide(title,points,speech,filename)\n",
    "    Slide.generate_video_from_slides([slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771240c2-9a18-46c8-99d2-a515afd30c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"test title\"\n",
    "points = [\"100\",\"200\",\"300\"]\n",
    "text = \"first is 100. second is 200. 3rd is 300. no more testing required.\"\n",
    "prediction(title,points,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599830b2-0ee9-4564-a73d-0cff6527d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crispy_shawarma/Graduation Project/AI-Service\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "currDirList = currDir.split('/')\n",
    "if(currDirList[-1] == \"notebooks\"):\n",
    "    newDir = \"/\".join(currDirList[:-1])\n",
    "    print(newDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b328e047-a9e0-4349-af6f-f94a240e3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_azure_blob(acc_name,acc_key):\n",
    "    # change curr directory to root if necessary\n",
    "    currDir = os.getcwd()\n",
    "    currDirList = currDir.split('/')\n",
    "    if(currDirList[-1] == \"notebooks\"):\n",
    "        newDir = \"/\".join(currDirList[:-1])\n",
    "        os.chdir(newDir)\n",
    "    from src.utils.azure_utils import AzureStorage\n",
    "    azure_storage_object = AzureStorage(acc_name,acc_key)\n",
    "    # return to current directory\n",
    "    os.chdir(currDir)\n",
    "    return azure_storage_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5a77138-af1c-4d53-bcc5-7028b0b1da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pydantic slides\n",
    "from pydantic import BaseModel, RootModel\n",
    "\n",
    "class SlidePydantic(BaseModel):\n",
    "    title: str\n",
    "    points: list[str]\n",
    "    text: str\n",
    "class ListSlidesPydantic (BaseModel):\n",
    "    slides: list[SlidePydantic]\n",
    "    def __iter__(self):\n",
    "        return iter(self.slides)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.slides[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e422b7f7-5bd7-44b7-ba0c-5cf69b2f4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing blob name:\n",
    "def store_blob(video_name,video_path):\n",
    "    blob_name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\") + video_name\n",
    "    azure_storage_object.store(container_name,blob_name,video_path)\n",
    "    return blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5c7bd8b-d004-4645-9d4e-d7cce9557ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sas link\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import BlobClient, generate_blob_sas, BlobSasPermissions\n",
    "def get_blob_sas(container_name, blob_name):\n",
    "    sas_blob = generate_blob_sas(account_name=acc_name, \n",
    "                                container_name=container_name,\n",
    "                                blob_name=blob_name,\n",
    "                                account_key=acc_key,\n",
    "                                permission=BlobSasPermissions(read=True),\n",
    "                                expiry=datetime.utcnow() + timedelta(hours=1))\n",
    "    return sas_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a14703a-3bd9-4e4d-be47-ce604c93b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "\n",
    "\n",
    "app = FastAPI(title='TTS_model')\n",
    "\n",
    "#setup azure blob connection\n",
    "\n",
    "acc_name = \"graduationproject19024\"\n",
    "acc_key = \"l6cWAFptkSjT9EI033DehgiVbVXojBFF89mAN+JN8ibNGk3jhB/TComT+FEf+w3YvuQoWSu7TGbT+AStQWWNZg==\"\n",
    "container_name = \"videos\"\n",
    "azure_storage_object = connect_azure_blob(acc_name,acc_key)\n",
    "\n",
    "# Get method endpoint\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return \"API is working correctly. Now head over to http://localhost:8000/docs.\"\n",
    "\n",
    "\n",
    "# post tts model endpoint\n",
    "@app.post(\"/tts\") \n",
    "def generate_video(slides:ListSlidesPydantic):\n",
    "    generated_slides = []\n",
    "    i = 1\n",
    "    for slide in slides:\n",
    "        print(type(slide))\n",
    "        #generate speech\n",
    "        speech = generate_speech(slide.text)\n",
    "        filename = f'slide {i}.wav'\n",
    "        i = i +1\n",
    "        write(filename, sr, speech)\n",
    "        # generate slides\n",
    "        slide = Slide(slide.title,slide.points,speech,filename)\n",
    "        generated_slides.append(slide)\n",
    "    video_name = \"output_video.mp4\"\n",
    "    video_path = video_name\n",
    "    Slide.generate_video_from_slides(generated_slides,video_name)\n",
    "    # create blob and store it on azure\n",
    "    blob_name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\") + video_name\n",
    "    azure_storage_object.store(container_name,blob_name,video_path)\n",
    "    # get sas for blob for public access\n",
    "    blob = get_blob_sas(container_name, blob_name)\n",
    "    url = 'https://'+acc_name+'.blob.core.windows.net/'+container_name+'/'+blob_name+'?'+blob\n",
    "    # Return file url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7549aaf8-3f19-42e9-b6b8-5bf4652926bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [7247]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SlidePydantic'>\n",
      "['first is 100.', ' second is 200.', ' 3rd is 300.', ' more testing required.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 36.55it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 49.41it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 44.62it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 72.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SlidePydantic'>\n",
      "['fourth is 400.', ' fifth is 500.', ' 6th is 600.', ' no more testing required.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 44.72it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 23.39it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.73it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 67.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.43\n",
      "6.21\n",
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n",
      "INFO:     127.0.0.1:54546 - \"POST /tts HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [7247]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOCKER-SETUP\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Spin up the server!    \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/uvicorn/main.py:575\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    573\u001b[0m     Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n\u001b[1;32m    577\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(config\u001b[38;5;241m.\u001b[39muds)  \u001b[38;5;66;03m# pragma: py-win32\u001b[39;00m\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/uvicorn/server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:360\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/uvicorn/server.py:68\u001b[0m, in \u001b[0;36mServer.serve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_signals\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Graduation Project/AI-Service/venv/lib/python3.11/site-packages/uvicorn/server.py:328\u001b[0m, in \u001b[0;36mServer.capture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# If we did gracefully shut down due to a signal, try to\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# trigger the expected behaviour now; multiple signals would be\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m captured_signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captured_signals):\n\u001b[0;32m--> 328\u001b[0m     signal\u001b[38;5;241m.\u001b[39mraise_signal(captured_signal)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Allows the server to be run in this interactive environment\n",
    "nest_asyncio.apply()\n",
    "\n",
    "host = \"0.0.0.0\" if os.getenv(\"DOCKER-SETUP\") else \"127.0.0.1\"\n",
    "\n",
    "# Spin up the server\n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
